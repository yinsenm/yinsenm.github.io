---
layout: post
title: STAT 640 Lecture 2-7
---
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<meta name="generator" content="http://www.nongnu.org/elyxer/"/>
<meta name="create-date" content="2014-10-26"/>
<link rel="stylesheet" href="{{ site.baseurl }}public/css/lyx.css">
<!-- <link rel="stylesheet" href="http://elyxer.nongnu.org/lyx.css" type="text/css" media="all"/> -->
<title>Converted document</title>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<div id="globalWrapper">
<script type="math/tex">
\newcommand{\lyxlock}{}
</script>
<noscript>
<div class="warning">
Warning: <a href="http://www.mathjax.org/">MathJax</a> requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
</div><hr/>
</noscript>
<div class="fulltoc">
<div class="tocheader">
Table of Contents
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-1">Section 1: Lecture 2</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-1.1">Subsection 1.1: Types of SL Problems:</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2">Subsection 1.2: Tasks:</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.3">Subsection 1.3: K-Nearest Neighbors (KNN)</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.4">Subsection 1.4: Training and Testing Idea: Future data</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-2">Section 2: Lecture 3</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-2.1">Subsection 2.1: Linear Regression</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsubsection-2.1.1">Subsubsection 2.1.1: Solving LS</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.2">Subsection 2.2: <span class="MathJax_Preview">X</span> is categorical</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-3">Section 3: Lecture 04</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-3.1">Subsection 3.1: Issues for least square</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2">Subsection 3.2: Ridege Regression (Tikhonor Regularization)</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsubsection-3.2.1">Subsubsection 3.2.1</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsubsection-3.2.2">Subsubsection 3.2.2</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.3">Subsection 3.3</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-4">Section 4: Lecture 05</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1">Subsection 4.1: Ridge Regression</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.2">Subsection 4.2: SVD PCA</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-4.3">Subsection 4.3: p<span class="MathJax_Preview">\gg</span>n</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-5">Section 5: Lecture 06</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-5.1">Subsection 5.1: Sparse Regression</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.2">Subsection 5.2: The Lasso Regularization Path (LARS)</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.3">Subsection 5.3: Solve LASSO Biased</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-6">Section 6: Lecture 07</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-6.1">Subsection 6.1: Sparse Regression</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsubsection-6.1.1">Subsubsection 6.1.1: Elastics Nets</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsubsection-6.1.2">Subsubsection 6.1.2: Comparison</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.2">Subsection 6.2: Classification</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3">Subsection 6.3: KNN: </a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4">Subsection 6.4: Nearest Centroid Classifier: </a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.5">Subsection 6.5: Naive Bayes Classifier:</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.6">Subsection 6.6: Linear Discriminant Analysis</a>
</div>
</div>
</div>

</div>
<h1 class="Section">
<a class="toc" name="toc-Section-1">1</a> Lecture 2
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.1">1.1</a> Types of SL Problems:
</h2>
<ol>
<li>
Supervised Learning<ol>
<li>
Have labels and outcomes <span class="MathJax_Preview"><script type="math/tex">
\{Y_{n\times1},X_{n\times p}\}
</script>
</span><ol>
<li>
<span class="MathJax_Preview"><script type="math/tex">
X_{n\times p}
</script>
</span> Data Matrix (given or fixed) 
</li>
<li>
<span class="MathJax_Preview"><script type="math/tex">
Y_{n\times1}
</script>
</span> outcomes: ordinal , quantitative (regression) , binary (classification) , categorical 
</li>

</ol>

</li>

</ol>

</li>
<li>
Unsupervised Learning <ol>
<li>
Clustering groups of features
</li>
<li>
Pattern Recognition 
</li>
<li>
Association between features
</li>

</ol>

</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.2">1.2</a> Tasks:
</h2>
<div class="Standard">
Prediction : Inference/Interpretation
</div>
<ol>
<li>
Base Learner: Algorithmic <ol>
<li>
Loss (Come from Probabilistic Model) + Penalty
</li>

</ol>

</li>
<li>
Ensemble Learning: Learning with groups of Base Learner
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.3">1.3</a> K-Nearest Neighbors (KNN)
</h2>
<ol>
<li>
Classification:<div class="Standard">
Predict Labels <span class="MathJax_Preview"><script type="math/tex">
x_{i}
</script>
</span> based on majority vote of <span class="MathJax_Preview"><script type="math/tex">
k
</script>
</span> nearest neighbors
</div>

</li>
<li>
Regression: - Predict ave of K nearest neighbors for <span class="MathJax_Preview"><script type="math/tex">
x_{i}
</script>
</span>
</li>

</ol>
<div class="Standard">
KNN is an example of Instance Based Learning. Distance<span class="FootOuter"><span class="SupFootMarker"> [A] </span><span class="HoverFoot"><span class="SupFootMarker"> [A] </span>http://www.mathworks.com/help/stats/pdist.html</span></span>: Euclidean
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
K=1
</script>
</span>is an Interpolating function or connect dots function.
</div>
<div class="Standard">
When <span class="MathJax_Preview"><script type="math/tex">
K
</script>
</span> is small, the model is complex. While <span class="MathJax_Preview"><script type="math/tex">
K
</script>
</span> is large, the model is simple.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.4">1.4</a> Training and Testing Idea: Future data
</h2>
<ol>
<li>
Overfitting: Fit training set well but terrible predicting future data.<div class="Standard">
Low training error - High testing error
</div>
<div class="Standard">
Curse of Dimensional <span class="MathJax_Preview"><script type="math/tex">
p
</script>
</span> increases, the data points are far apart (because of adding some irrelevant noises).
</div>

</li>

</ol>
<h1 class="Section">
<a class="toc" name="toc-Section-2">2</a> Lecture 3
</h1>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
E\left[\left(\hat{y}-f\left(x\right)\right)^{2}\right] & = & E\left[\left(\hat{y}-E\left[\hat{y}\right]\right)-\left(f\left(x\right)-E\left[\hat{y}\right]\right)^{2}\right]\\
 & = & E\left[\left(\hat{y}-E\left[\hat{y}\right]\right)^{2}\right]-E\left[\left(f\left(x\right)-E\left[\hat{y}\right]\right)^{2}\right]\\
 & = & Var\left(\hat{y}\right)+Bias^{2}\left(\hat{y}\right)\\
E\left[\left(\hat{y}-y\right)^{2}\right] & = & Var\left(\hat{y}\right)+Bias^{2}\left(\hat{y}\right)+\sigma^{2}
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
KNN Regression
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
K
</script>
</span> small, Low Bias High variance, when <span class="MathJax_Preview"><script type="math/tex">
K=1
</script>
</span>varance Explodes
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
K
</script>
</span> large, High Bias Low variance
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-2.1">2.1</a> Linear Regression
</h2>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
Y & = & f\left(x\right)+\epsilon\\
 & = & \beta_{0}+X\beta_{p\times1}+\epsilon\\
 & = & \tilde{X}\tilde{\beta}+\epsilon
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
where <span class="MathJax_Preview"><script type="math/tex">
\epsilon
</script>
</span> is in reducible noise, <span class="MathJax_Preview"><script type="math/tex">
\beta_{p\times1}
</script>
</span> is coefficent vector and <span class="MathJax_Preview"><script type="math/tex">
\tilde{X}=\left[\underline{1},X\right]
</script>
</span>.
</div>
<h3 class="Subsubsection">
<a class="toc" name="toc-Subsubsection-2.1.1">2.1.1</a> Solving LS
</h3>
<div class="Standard">
Minimize the RSS
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}

</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\dfrac{\partial}{\partial\beta} & = & -X^{T}\left(Y-X\beta\right)=0\\
X^{T}X\beta & = & X^{T}Y\\
\hat{\beta} & = & \left(X^{T}X\right)^{-1}X^{T}Y\\
\hat{Y} & = & X\hat{\beta}\\
 & = & X\left(X^{T}X\right)^{-1}X^{T}Y\\
 & = & HY
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
where <span class="MathJax_Preview"><script type="math/tex">
H
</script>
</span> is a hat matrix and a projection matrix who has eigen values <span class="MathJax_Preview"><script type="math/tex">
\left\{ 0,1\right\} 
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
H^{2}=H
</script>
</span>.
</div>
<div class="Standard">
Statistics Properties
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\hat{\sigma}^{2} & = & \dfrac{1}{n-p-1}\sum_{i=1}^{n}\left(y_{i}-\hat{y_{i}}\right)^{2}\\
\sigma_{MLE}^{2} & = & \dfrac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\hat{y_{i}}\right)^{2}
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\hat{\beta} & \sim & N\left(\beta,\left(X^{T}X\right)^{-1}\sigma^{2}\right)\\
E\left[\hat{\beta}\right] & = & \beta\\
Var\left(\hat{\beta}\right) & = & \left(X^{T}X\right)^{-1}\sigma^{2}\\
\hat{\beta_{j}}/\hat{\sigma}\sqrt{\left(X^{T}X\right)_{j\times j}^{-1}} & \sim & t_{n-p-1}\\
 &  & \chi_{n-p-1}\\
 &  & F
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
Gauss-Markov Thm, <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}^{ls}
</script>
</span> is the best linear unbiased estimator BLUE <span class="MathJax_Preview"><script type="math/tex">
Var\left(\hat{\beta}^{ls}\right)\leq Var\left(\beta^{LUE}\right)
</script>
</span>.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-2.2">2.2</a> <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> is categorical
</h2>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
x_{j}
</script>
</span>: <span class="MathJax_Preview"><script type="math/tex">
k
</script>
</span> categories can be changed into indicating matrix using dummy variables. This approach is similar to ANOVA.
</div>
<div class="Standard">
Colinearity: conditionally independently. <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> becomes close to singular and ill condition for computation. 
</div>
<div class="Standard">
If <span class="MathJax_Preview"><script type="math/tex">
X_{i}
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
X_{j}
</script>
</span> are colinear, then each <span class="MathJax_Preview"><script type="math/tex">
\beta_{i}
</script>
</span> or <span class="MathJax_Preview"><script type="math/tex">
\beta_{j}
</script>
</span> will be large while the other will be small. And Largely inflat variance of MSE.
</div>
<div class="Standard">
When <span class="MathJax_Preview"><script type="math/tex">
p\gg n
</script>
</span>,
</div>
<div class="Standard">
when <span class="MathJax_Preview"><script type="math/tex">
p>n
</script>
</span>, training errors would be<span class="MathJax_Preview"><script type="math/tex">
RSS=0
</script>
</span>. <b>Proof</b>
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-3">3</a> Lecture 04
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-3.1">3.1</a> Issues for least square
</h2>
<div class="Standard">
Colinearity: Correlated predictors.
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
p>n
</script>
</span> or <span class="MathJax_Preview"><script type="math/tex">
rank\left(M\right)>n
</script>
</span>, then <span class="MathJax_Preview"><script type="math/tex">
RSS=0
</script>
</span> massively overfit!
</div>
<div class="Standard">
Computational Issues <span class="MathJax_Preview"><script type="math/tex">
\left(X^{T}X\right)^{-1}
</script>
</span>, Simple Fix: Variable Selection.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-3.2">3.2</a> Ridege Regression (Tikhonor Regularization)
</h2>
<div class="Standard">
Idea: introduce some bias and decrease the variance
</div>
<h3 class="Subsubsection">
<a class="toc" name="toc-Subsubsection-3.2.1">3.2.1</a> <b>Ridge (Constrainted)</b>
</h3>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2} & \text{sub to} & \left\Vert \beta\right\Vert _{2}^{2}\le t
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
where t contrains the magnitude of <span class="MathJax_Preview"><script type="math/tex">
\beta
</script>
</span>s
</div>
<h3 class="Subsubsection">
<a class="toc" name="toc-Subsubsection-3.2.2">3.2.2</a> <b>Ridge (Lagrange)</b>
</h3>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{equation}
\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}+\lambda\left\Vert \beta\right\Vert _{2}^{2}\label{eq:1}
\end{equation}
</script>

</span>

</div>
<div class="Standard">
Loss function + Penalty. A trade off between model fitting and constraints. <span class="MathJax_Preview"><script type="math/tex">
\lambda\ge0
</script>
</span> &ldquo;penalty&rdquo; parameter regularization.
</div>
<div class="Standard">
One to One mapping between <span class="MathJax_Preview"><script type="math/tex">
\lambda
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
t
</script>
</span>. Constraint and Lagrange
</div>
<div class="Standard">
When <span class="MathJax_Preview"><script type="math/tex">
\lambda=0
</script>
</span>, Loss function is the least square.
</div>
<div class="Standard">
When <span class="MathJax_Preview"><script type="math/tex">
\lambda\rightarrow\infty
</script>
</span>, <span class="MathJax_Preview"><script type="math/tex">
\beta
</script>
</span>s are massively shrunked.
</div>
<div class="Standard">
Find the sollution to <a class="Reference" href="#eq:1">↓</a>, 
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\dfrac{\partial}{\partial\beta} & = & -X^{T}\left(Y-X\beta\right)+2\lambda\beta=0\\
\hat{\beta} & = & \left(X^{T}X+\lambda I\right)^{-1}X^{T}Y
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
The original OLS matrix <span class="MathJax_Preview"><script type="math/tex">
\left(X^{T}X\right)^{-1}
</script>
</span> becomes more invertible and <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}
</script>
</span> is always a unique solution.
</div>
<div class="Standard">
If <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> orthogonal <span class="MathJax_Preview"><script type="math/tex">
X^{T}X=I
</script>
</span>, then we can see that it is a &ldquo;Shrinkage&rdquo; Estimator. We add bias to lower the variance.
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\hat{\beta}^{R} & = & \left(X^{T}X+\lambda I\right)^{-1}X^{T}Y\\
 & = & \left(X^{T}Y\right)/\left(\lambda+1\right)\\
 & = & \hat{\beta}^{LS}/\left(\lambda+1\right)
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
Bias\left(\hat{\beta}^{R}\right) & = & -\left(X^{T}X+\lambda I\right)^{-1}X^{T}X\beta\\
Var\left(\hat{\beta}^{R}\right) & = & \sigma^{2}\left(X^{T}X+\lambda I\right)^{-1}X^{T}X\left(X^{T}X+\lambda I\right)^{-1}\\
Var\left(\hat{\beta}^{LS}\right) & = & \sigma^{2}\left(X^{T}X\right)^{-1}\\
\forall\lambda,Var\left(\hat{\beta}^{R}\right) & \le & Var\left(\hat{\beta}^{LS}\right)\text{proof!}
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
Existence Thm:
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\exists\lambda,\text{ s.t. }MSE\left(\hat{\beta}^{R}\right)\le MSE\left(\hat{\beta}^{LS}\right)

</script>

</span>

</div>
<div class="Standard">
Interpretation: 
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
\beta
</script>
</span>s are shrunked. 
</div>
<div class="Standard">
Intercept: typitically we don’t penalize the intercept. or center the <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span>. 
</div>
<div class="Standard">
Scaling: if not scale, the regularization depends on the scale of <span class="MathJax_Preview"><script type="math/tex">
\lambda_{i}
</script>
</span>. Then we shrunk each features differently. 
</div>
<div class="Standard">
If scale, it is the same regularization for all features and treat each features fairly.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-3.3">3.3</a> <b>SVD</b>
</h2>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
X & = & UDV^{T}
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
rank\left(X\right)=r
</script>
</span>
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\hat{\beta}^{R} & = & \left(X^{T}X+\lambda I\right)^{-1}X^{T}Y\\
 & = & \left(VDU^{T}UDV^{T}+\lambda I\right)^{-1}VDU^{T}Y\\
 & = & \left(VD^{2}V^{T}+\lambda I\right)^{-1}VDU^{T}Y\\
 & = & V\left(D^{2}+\lambda I\right)^{-1}V^{T}VDU^{T}Y\\
 & = & V\left(D^{2}+\lambda I\right)^{-1}DU^{T}Y\\
\hat{y} & = & X\hat{\beta}^{R}\\
 & = & UDV^{T}\left[V\left(D^{2}+\lambda I\right)DU^{T}Y\right]\\
 & = & UD\left(D^{2}+\lambda I\right)^{-1}DU^{T}Y\\
 & = & \sum_{j=1}^{p}u_{j}\left(\dfrac{d_{j}^{2}}{d_{j}^{2}+\lambda}\right)u_{j}^{T}y
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
U
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
V
</script>
</span> capture major patterns. Major correlation in <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> is groupped together or shrunked together. Ridege Regression is the &ldquo;best&rdquo; method to use for colinearity.
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-4">4</a> Lecture 05
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.1">4.1</a> Ridge Regression
</h2>
<div class="Standard">
Penalty <span class="MathJax_Preview"><script type="math/tex">
P\left(\beta\right)=\left\Vert \beta\right\Vert _{2}^{2}
</script>
</span>
</div>
<div class="Standard">
Advantage: 
</div>
<ol>
<li>
co-linearity (<span class="MathJax_Preview"><script type="math/tex">
corX
</script>
</span>)
</li>
<li>
Prediction <span class="MathJax_Preview"><script type="math/tex">
\exists\lambda
</script>
</span> s.t. <span class="MathJax_Preview"><script type="math/tex">
MSE\left[\hat{\beta}^{R}\right]\le MSE\left[\hat{\beta}^{LS}\right]
</script>
</span>. 
</li>
<li>
Computational. The ridge regression always has a solution.
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.2">4.2</a> SVD PCA
</h2>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
rank\left(X\right)=r
</script>
</span>
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
X_{n\times p} & = & U_{n\times r}D_{r\times r}V_{p\times r}^{T}
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
where <span class="MathJax_Preview"><script type="math/tex">
D
</script>
</span> is a diag matrix with singular values. <span class="MathJax_Preview"><script type="math/tex">
d_{1}\ge d_{2}\ge\cdots\ge d_{r}
</script>
</span>, <span class="MathJax_Preview"><script type="math/tex">
U
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
V
</script>
</span> are orthonormal or <span class="MathJax_Preview"><script type="math/tex">
U^{T}U=I
</script>
</span>. The cols of <span class="MathJax_Preview"><script type="math/tex">
U
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
V
</script>
</span> are sets of orthonormal linear directions that maximize the variance in rows (obs) or cols (features). Goal: maximize variance projection of data by <span class="MathJax_Preview"><script type="math/tex">
V
</script>
</span>. Columns of <span class="MathJax_Preview"><script type="math/tex">
U
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
V
</script>
</span> (direction vector) capture the major variance or major information in the data. 
</div>
<div class="Standard">
Regression on
</div>
<ol>
<li>
Derived features.<ol>
<li>
PC Regression (Idea: variation in <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> is useful for predicting <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span>)<ol>
<li>
Take SVD and find <span class="MathJax_Preview"><script type="math/tex">
Z=UD
</script>
</span> (truncated <span class="MathJax_Preview"><script type="math/tex">
k<r
</script>
</span>)
</li>
<li>
regression on Z<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\hat{\beta}^{PCR} & = & \left(Z^{T}Z\right)^{-1}Z^{T}y\\
 & = & \left(DU^{T}UD\right)^{-1}DU^{T}y\\
 & = & D^{-1}U^{T}y
\end{eqnarray*}
</script>

</span>

</li>

</ol>

</li>
<li>
PLS Partial Least Square Regression<ol>
<li>
PCA maximize <span class="MathJax_Preview"><script type="math/tex">
Cov\left(x\right)
</script>
</span> while PLS maximize <span class="MathJax_Preview"><script type="math/tex">
Cov\left(X,y\right)
</script>
</span>
</li>
<li>
Find direction variation in <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> related to <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span>.
</li>

</ol>

</li>

</ol>

</li>
<li>
Dictionaries.
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.3">4.3</a> p<span class="MathJax_Preview"><script type="math/tex">
\gg
</script>
</span>n
</h2>
<ol>
<li>
Look at largest coefs
</li>
<li>
Feature selections<ol>
<li>
<span class="MathJax_Preview"><script type="math/tex">
\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}
</script>
</span> sub to <span class="MathJax_Preview"><script type="math/tex">
\left\Vert \beta\right\Vert _{0}\le K
</script>
</span> <span class="MathJax_Preview"><script type="math/tex">
L_{0}
</script>
</span> norm <span class="MathJax_Preview"><script type="math/tex">
\left|\left\{ \beta\right\} \right|
</script>
</span> number of coeffs of non-zeros <span class="MathJax_Preview"><script type="math/tex">
\beta
</script>
</span>s
</li>
<li>
But the problem is <span class="MathJax_Preview"><script type="math/tex">
NP
</script>
</span> hard. We have <span class="MathJax_Preview"><script type="math/tex">
\binom{p}{k}
</script>
</span> coefs to choose from <span class="MathJax_Preview"><script type="math/tex">
p
</script>
</span><span class="MathJax_Preview"><script type="math/tex">
\beta
</script>
</span>s
</li>
<li>
Stepwise regression:<ol>
<li>
Forward:<ol>
<li>
Find the best predictor add to model
</li>
<li>
given A. find the next best. (selected by <span class="MathJax_Preview"><script type="math/tex">
AIC=\left\Vert y-X\hat{\beta}\right\Vert _{2}^{2}+2df\left(\hat{\beta}\right)
</script>
</span> <span class="MathJax_Preview"><script type="math/tex">
df\left(\hat{\beta}\right)=tr\left(H\right)
</script>
</span>  BIC)
</li>
<li>
Computation intense for large <span class="MathJax_Preview"><script type="math/tex">
p
</script>
</span> and bad for correlation data.
</li>

</ol>

</li>
<li>
Backward:
</li>

</ol>

</li>
<li>
<span class="MathJax_Preview"><script type="math/tex">
L_{1}
</script>
</span> regression <span class="MathJax_Preview"><script type="math/tex">
\left(Lasso\right)
</script>
</span> solution <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}\text{sub to }\left\Vert \beta\right\Vert _{1}\le1

</script>

</span>
or Lagrange form<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}+\left\Vert \beta\right\Vert _{1}

</script>

</span>

</li>

</ol>

</li>

</ol>
<h1 class="Section">
<a class="toc" name="toc-Section-5">5</a> Lecture 06
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.1">5.1</a> Sparse Regression
</h2>
<div class="Standard">
To solve <span class="MathJax_Preview"><script type="math/tex">
L_{0}
</script>
</span> problem or best subsets problem, we want to solve <span class="MathJax_Preview"><script type="math/tex">
\min_{k}\left\Vert Y-X\beta\right\Vert _{2}^{2}
</script>
</span> with constraint <span class="MathJax_Preview"><script type="math/tex">
\left\Vert \beta\right\Vert _{0}\le k
</script>
</span>. When <span class="MathJax_Preview"><script type="math/tex">
\lambda
</script>
</span> large, we have sparse <span class="MathJax_Preview"><script type="math/tex">
\beta
</script>
</span>s. While <span class="MathJax_Preview"><script type="math/tex">
\lambda\rightarrow0
</script>
</span>, we approach to LS. Ridge regression only shrinks the <span class="MathJax_Preview"><script type="math/tex">
\beta s
</script>
</span>. When <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> is orthogonal or <span class="MathJax_Preview"><script type="math/tex">
X^{T}X=I
</script>
</span>, <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta^{LS}}=X^{T}Y
</script>
</span>, <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}^{R}=\dfrac{\hat{\beta}^{LS}}{\lambda+1}
</script>
</span>, <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}^{Lasso}=S\left(\hat{\beta}^{LS},\lambda\right)
</script>
</span> &ldquo;Soft- thresholding&rdquo; is also a shrinkage estimator.
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

S\left(x,\lambda\right)=sign\left(x\right)\left(\left|x\right|-\lambda\right)=\max\left(\left|x\right|-\lambda,0\right)=\begin{cases}
0 & \left|x\right|<\lambda\\
x-\lambda & x>\lambda\\
\lambda+x & x<\lambda
\end{cases}

</script>

</span>

</div>
<div class="Standard">
<b>Thm:</b> <span class="MathJax_Preview"><script type="math/tex">
L_{1}
</script>
</span>-norm is the best convex relaxation of the <span class="MathJax_Preview"><script type="math/tex">
L_{0}
</script>
</span>-norm. (-easy to compute, -optimal solution)
</div>
<div class="Standard">
Convex Problem:We can find the global optimum but need strict convexity. If not strictly convex, the optimum estimator doesn’t uniquely exist. 
</div>
<div class="Standard">
Our lasso problem is not strictly convex, which means that no great for coefficients interpretation, <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}^{Lasso}
</script>
</span> biased , and no hypothesis testing.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.2">5.2</a> The Lasso Regularization Path (LARS)
</h2>
<div class="Standard">
Their regularization path is piece wise linear.
</div>
<div class="Standard">
Least Angle Regression: (similar to OMP)
</div>
<ol>
<li>
Start with <span class="MathJax_Preview"><script type="math/tex">
X_{j}
</script>
</span> most correlates with <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span>.
</li>
<li>
Move in the least square direction for <span class="MathJax_Preview"><script type="math/tex">
X_{j}
</script>
</span> (move along <span class="MathJax_Preview"><script type="math/tex">
\beta_{j}
</script>
</span> sloop) until another variable is equally correlated with current residual <span class="MathJax_Preview"><script type="math/tex">
r=Y-X_{j}\beta_{j}
</script>
</span>.
</li>
<li>
Add var to the model <span class="MathJax_Preview"><script type="math/tex">
\left(X_{k}\right)
</script>
</span> and move in joint least square direction <span class="MathJax_Preview"><script type="math/tex">
\left(X_{j},X_{k}\right)
</script>
</span>.
</li>
<li>
Repeat until all vars (or at least square solutions).
</li>

</ol>
<div class="Standard">
LARS does not always have the same solution as LASSO. LASSO’s correction: if a coef path hits 0, drop it form the model and recompute the joint least square direction. The difference happens when there is high col linearity in the data or <span class="MathJax_Preview"><script type="math/tex">
p\gg n
</script>
</span>. <span class="MathJax_Preview"><script type="math/tex">
\exists\lambda_{\max}=\max X^{T}Y
</script>
</span> s.t. <span class="MathJax_Preview"><script type="math/tex">
\beta\left(\lambda_{\max}\right)\equiv0
</script>
</span>.
</div>
<ol>
<li>
Semi Def Programming (1996)
</li>
<li>
LARS (2001)
</li>
<li>
Coordinate descent / shooting (2007) in ESL 
</li>
<li>
Proximal Gradient (2009) (Iterate shrinkage and thesholding algorithm)<ol>
<li>
<span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}^{\left(k\right)}=S\left(\hat{\beta}^{\left(k-1\right)}-\dfrac{1}{L}\nabla l\left(\hat{\beta}^{\left(k-1\right)},\lambda/L\right)\right)
</script>
</span> <span class="MathJax_Preview"><script type="math/tex">
L=\lambda_{max}\left(X^{T}X\right)-X^{T}\left(Y-X\hat{\beta}^{\left(k-1\right)}\right)
</script>
</span>?
</li>

</ol>

</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.3">5.3</a> Solve LASSO Biased
</h2>
<ol>
<li>
Since the parameters of LASSO is greatly shrinkage (biased parameters), we don’t want to use those variables for prediction purpose, but use LASSO for variable selection and go back to refit LS. 
</li>
<li>
Adaptive Lasso(another way to solve the biased):<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}+\lambda\sum_{i}\hat{w_{j}}\left|\beta_{j}\right|

</script>

</span>
<span class="MathJax_Preview"><script type="math/tex">
\beta_{j}
</script>
</span> increases, <span class="MathJax_Preview"><script type="math/tex">
w_{j}
</script>
</span> decreases such that less shrinkage. One chosen of weights can be: <span class="MathJax_Preview"><script type="math/tex">
\hat{w}_{j}=1/\hat{\beta}_{j}^{LS}
</script>
</span> or <span class="MathJax_Preview"><script type="math/tex">
1/\hat{\beta}_{j}^{R}
</script>
</span> or <span class="MathJax_Preview"><script type="math/tex">
1/\hat{\beta}_{j}^{Lasso}
</script>
</span>.
</li>
<li>
SCAD (Smoothly Clipped Abr Deviation) <br/>
SCAD: Smooth link between soft threshold and least square.
</li>
<li>
MC+: Smooth link between hard-threshold and least square.
</li>

</ol>
<h1 class="Section">
<a class="toc" name="toc-Section-6">6</a> Lecture 07
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.1">6.1</a> Sparse Regression
</h2>
<ol>
<li>
Feature Selection<ol>
<li>
Lasso: <span class="MathJax_Preview"><script type="math/tex">
\min\left\Vert Y-X\beta\right\Vert _{2}^{2}+\lambda\left\Vert \beta\right\Vert _{1}
</script>
</span> 
</li>
<li>
SCAD, MC+ (non convex) Adaptive Lasso
</li>

</ol>

</li>
<li>
Lasso tends to pick one variable out of a correlated group<span class="MathJax_Preview">
<script type="math/tex;mode=display">

r_{1}=Y-X\hat{\beta}_{1}

</script>

</span>
Ok for prediction but not ok for inter prediction.
</li>
<li>
Theory: Only get sparsistent <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}^{Lasso}
</script>
</span>. <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\left\Vert X_{S^{C}}^{T}X_{S}\left(X_{S}^{T}X_{S}\right)^{-1}\right\Vert _{\infty}\le1-\eta\rightleftarrows P\left(correct\ \beta\ support\right)\rightarrow1

</script>

</span>
<ol>
<li>
Irrep says<ol>
<li>
True vars cannot be top
</li>

</ol>

</li>

</ol>

</li>

</ol>
<h3 class="Subsubsection">
<a class="toc" name="toc-Subsubsection-6.1.1">6.1.1</a> Elastics Nets
</h3>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\beta}\left\Vert Y-X\beta\right\Vert _{2}^{2}+\lambda\alpha\left\Vert \beta\right\Vert _{1}+\lambda\left(1-\alpha\right)\left\Vert \beta\right\Vert _{2}^{2}

</script>

</span>

</div>
<div class="Standard">
where <span class="MathJax_Preview"><script type="math/tex">
\alpha\in\left[0,1\right]
</script>
</span>. It tends to select group of variables together. 
</div>
<h3 class="Subsubsection">
<a class="toc" name="toc-Subsubsection-6.1.2">6.1.2</a> Comparison
</h3>
<ol>
<li>
Prediction: Ridge regression<div class="Standard">
Compared to Lasso, Ridge is a great way to do prediction, because ridge shrink the parameters as a group for prediction.
</div>

</li>
<li>
Feature selection: <div class="Standard">
Interpreting Coef: SCAD, MC+, AdLasso
</div>
<div class="Standard">
Col linearity: E NET 
</div>
<div class="Standard">
Prediction: Lasso
</div>

</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.2">6.2</a> Classification
</h2>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
Y_{n\times1}
</script>
</span> class labels
</div>
<div class="Standard">
Goal: <span class="MathJax_Preview"><script type="math/tex">
X_{n\times p}
</script>
</span> to predict <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span> labels.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.3">6.3</a> KNN: 
</h2>
<div class="Standard">
fails at high dimensional setting
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.4">6.4</a> Nearest Centroid Classifier: 
</h2>
<ol>
<li>
How to define centroid? <span class="MathJax_Preview"><script type="math/tex">
\hat{\mu}_{k}=\dfrac{1}{n}\sum_{i\in C\left(k\right)}X_{i}
</script>
</span> <span class="MathJax_Preview"><script type="math/tex">
C\left(k\right)=\left\{ i\mid i\in class\; k\right\} 
</script>
</span>
</li>
<li>
How to define distance? We use Euclidean distance.
</li>
<li>
What do the decision boundary looks like? Look at the notes.
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.5">6.5</a> Naive Bayes Classifier:
</h2>
<div class="Standard">
Idea: use prob model for each class and then classify each observation to class with highest density.
</div>
<ol>
<li>
Prob Model for each class:<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
f_{K}\left(x\right) & = & P\left(X\mid Y=k\right)\\
x\mid Y & \sim & N\left(\mu_{k},\sigma^{2}\right)
\end{eqnarray*}
</script>

</span>

</li>
<li>
Class Rules<span class="MathJax_Preview">
<script type="math/tex;mode=display">

P\left(Y=k\mid X\right)=\dfrac{P\left(X\mid Y=k\right)}{\sum_{k}P\left(X\mid Y=k\right)}

</script>

</span>
Weighted class priors<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
\pi_{k} & = & P\left(Y\in C\left(k\right)\right)\\
\hat{\pi}_{k} & = & \dfrac{n_{k}}{n}
\end{eqnarray*}
</script>

</span>
such that<span class="MathJax_Preview">
<script type="math/tex;mode=display">

P\left(Y=k\mid X\right)=\dfrac{\pi_{k}P\left(X\mid Y=k\right)}{\sum_{k}\pi_{k}P\left(X\mid Y=k\right)}

</script>

</span>

</li>
<li>
Parameter estimation:<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{\mu}_{k}=\dfrac{1}{n_{k}}\sum_{i\in C\left(k\right)}X_{i}

</script>

</span>
which is MLE for Bayes Classifier Centroid
</li>
<li>
Decision Rule<ol>
<li>
Class size equal: exactly the same for the nearest centroid classifier.
</li>
<li>
Class size unequal: shifted decision lines towards weights.
</li>

</ol>

</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.6">6.6</a> Linear Discriminant Analysis
</h2>
<div class="Standard">
Multivariate Normal as Class Prob <span class="MathJax_Preview"><script type="math/tex">
X\mid Y\sim N\left(\mu_{k},\Sigma\right)
</script>
</span> and assume cov the same for all classes.
</div>
<div class="Standard">
Bayes Class Rule <span class="MathJax_Preview"><script type="math/tex">
\pi_{k}
</script>
</span>
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

f_{k}\left(x\right)=\dfrac{1}{\left(2\pi\right)^{n/2}\left|\Sigma\right|}\exp\left[-\dfrac{1}{2}\left(X-\mu_{k}\right)^{T}\Sigma^{-1}\left(X-\mu_{k}\right)\right]

</script>

</span>

</div>
<div class="Standard">
Find the MLE for <span class="MathJax_Preview"><script type="math/tex">
\pi_{k},\mu_{k}
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
\Sigma
</script>
</span>.
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{\Sigma}_{k}=\dfrac{1}{n-k}\sum_{k}\sum_{i\in C\left(k\right)}\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{T}

</script>

</span>

</div>
<div class="Standard">
Decision boundary
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
P\left(X=k\right) & = & P\left(X=k^{'}\right)\\
\log P\left(X=k\right) & = & \log P\left(X=k^{'}\right)\\
 & \sim & \Sigma^{-1}\left(X-\mu_{k}\right)
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
which is a linear function of <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span>.
</div>

<hr class="footer"/>
<div class="footer" id="generated-by">
Document generated by <a href="http://elyxer.nongnu.org/">eLyXer 1.2.5 (2013-03-10)</a> on <span class="create-date">2014-10-26T14:20:30.572000</span>
</div>
</div>
</body>
</html>
