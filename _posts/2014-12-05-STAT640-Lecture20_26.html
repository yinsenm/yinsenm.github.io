---
layout: post
title: STAT 640 Lecture 20-26
---
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8"/>
<meta name="generator" content="http://www.nongnu.org/elyxer/"/>
<meta name="create-date" content="2014-12-06"/>
<link rel="stylesheet" href="{{ site.baseurl }}public/css/lyx.css">
<title>Converted document</title>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
</head>
<body>
<div id="globalWrapper">
<script type="math/tex">
\newcommand{\lyxlock}{}
</script>
<noscript>
<div class="warning">
Warning: <a href="http://www.mathjax.org/">MathJax</a> requires JavaScript to correctly process the mathematics on this page. Please enable JavaScript on your browser.
</div><hr/>
</noscript>
<div class="fulltoc">
<div class="tocheader">
Table of Contents
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Section-1">Section 1: Lecture 20</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-1.1">Subsection 1.1: K means Review</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.2">Subsection 1.2: Hierarchical Clustering</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.3">Subsection 1.3: Biclustering</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-1.4">Subsection 1.4: Convex Clustering</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-2">Section 2: Lecture 21 (Graphic Models)</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-2.1">Subsection 2.1: Markov Network</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-2.2">Subsection 2.2: Gaussian Graphical Model (GGM)</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-3">Section 3: Lecture 22 CART (Classification and Regression Tree)</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-3.1">Subsection 3.1: Regression Tree</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-3.2">Subsection 3.2: Classification Tree</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-4">Section 4: Lecture 23</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-4.1">Subsection 4.1: Ensemble Learning</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsubsection-4.1.1">Subsubsection 4.1.1: Bagging</a>
</div>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-5">Section 5: Lecture 24</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-5.1">Subsection 5.1: Bagging</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.2">Subsection 5.2: OOB (Out of Bag) Error</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.3">Subsection 5.3: Boosting (1996)</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.4">Subsection 5.4: Adaboost (Discrete Verizon)</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-5.5">Subsection 5.5: Forward Stagewise Additive Modeling</a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-6">Section 6: Lecture 25</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-6.1">Subsection 6.1: Boosting</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.2">Subsection 6.2: Boosting Algs (Adaboost)</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.3">Subsection 6.3: Types of Boosters</a>
</div>
<div class="toc">
<a class="Link" href="#toc-Subsection-6.4">Subsection 6.4: Random Forest </a>
</div>
</div>
<div class="toc">
<a class="Link" href="#toc-Section-7">Section 7: Lecture 26</a>
</div>
<div class="tocindent">
<div class="toc">
<a class="Link" href="#toc-Subsection-7.1">Subsection 7.1: Model Stacking</a>
</div>
</div>
</div>

</div>
<h1 class="Section">
<a class="toc" name="toc-Section-1">1</a> Lecture 20
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.1">1.1</a> K means Review
</h2>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
W\left(C\right)
</script>
</span> minimize within cluster distance using Euclidean distance.
</div>
<ol>
<li>
Under downhill to local minimum. (dependent on initial value)
</li>
<li>
Difficulty in choosing <span class="MathJax_Preview"><script type="math/tex">
K
</script>
</span>.
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.2">1.2</a> Hierarchical Clustering
</h2>
<div class="Standard">
A complete algorithmic process (Agglomeration)
</div>
<div class="Standard">
Finds groups by binary joins b/w items.
</div>
<div class="Standard">
Dendogram (Nested Tree)
</div>
<div class="Standard">
Denote <span class="MathJax_Preview"><script type="math/tex">
d\left(x_{i},x_{i}^{'}\right)
</script>
</span> as the dissimilarity between <span class="MathJax_Preview"><script type="math/tex">
x_{i}
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
x_{i}^{'}
</script>
</span>.
</div>
<div class="Standard">
The algorithm is 
</div>
<ol>
<li>
Calculate the dissimilarity matrix <span class="MathJax_Preview"><script type="math/tex">
D
</script>
</span><span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{i,i^{'}}d\left(x_{i},x_{i^{'}}\right)

</script>

</span>
then join <span class="MathJax_Preview"><script type="math/tex">
\left(i,i^{'}\right)\in S_{1}
</script>
</span>
</li>
<li>
Step <span class="MathJax_Preview"><script type="math/tex">
k
</script>
</span> where <span class="MathJax_Preview"><script type="math/tex">
k
</script>
</span> from <span class="MathJax_Preview"><script type="math/tex">
2
</script>
</span> to <span class="MathJax_Preview"><script type="math/tex">
n-1
</script>
</span> <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\left\{ S_{1},S_{n-k}\right\} }linkage\left(d\left(S_{j},S_{j^{'}}\right)\right)

</script>

</span>
<ol>
<li>
Single Linkage <span class="MathJax_Preview"><script type="math/tex">
O\left(n\log n\right)
</script>
</span><span class="MathJax_Preview">
<script type="math/tex;mode=display">

d_{SL}\left(S_{j},S_{j^{'}}\right)=\min_{i\in S_{i},i^{'}\in S_{i^{'}}}d\left(x_{i},x_{i^{'}}\right)

</script>

</span>
can cause <b>chaining</b> in the higher dimension data set.
</li>
<li>
Average Linkage <span class="MathJax_Preview"><script type="math/tex">
O\left(n^{2}\log n\right)
</script>
</span><span class="MathJax_Preview">
<script type="math/tex;mode=display">

d_{Ave}\left(S_{j},S_{j^{'}}\right)=\dfrac{1}{n_{j}n_{j^{'}}}\sum_{i\in S_{j}}\sum_{i^{'}\in S_{j^{'}}}d\left(x_{i},x_{i^{'}}\right)

</script>

</span>

</li>
<li>
Central Linkage
</li>
<li>
Complete Linkage (minmax linkage)<span class="MathJax_Preview">
<script type="math/tex;mode=display">

d_{COM}\left(S_{j},S_{j^{'}}\right)=\max_{i\in S_{i},i^{'}\in S_{i^{'}}}d\left(x_{i},x_{i^{'}}\right)

</script>

</span>

</li>
<li>
Wards Linkage
</li>

</ol>

</li>

</ol>
<div class="Standard">
Note the height of the tree is proportion to euclidean distance in the dissimilar matrix.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.3">1.3</a> Biclustering
</h2>
<div class="Standard">
Group both in observation space and feature spaces.
</div>
<div class="Standard">
Overlaping
</div>
<div class="Standard">
Non-Overlaping (check board type)
</div>
<div class="Standard">
Cluster Dendogram: apply hierarchy model independently on rows and columns.
</div>
<div class="Standard">
Better way such as Sparse PCA, Sparse SVD or etc.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-1.4">1.4</a> Convex Clustering
</h2>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

X_{n\times p}=U_{n\times p}

</script>

</span>

</div>
<div class="Standard">
where each row of <span class="MathJax_Preview"><script type="math/tex">
U_{n\times p}
</script>
</span> represent class centroids. We want to join the rows of <span class="MathJax_Preview"><script type="math/tex">
U
</script>
</span> together or want to squeeze <span class="MathJax_Preview"><script type="math/tex">
u_{i}
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
u_{i^{'}}
</script>
</span> together.
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{U}\left\Vert X-U\right\Vert _{F}^{2}+\gamma\sum_{i<j}^{n}w_{ij}\left\Vert u_{i}-u_{j}\right\Vert _{1}

</script>

</span>

</div>
<h1 class="Section">
<a class="toc" name="toc-Section-2">2</a> Lecture 21 (Graphic Models)
</h1>
<div class="Standard">
Learn the relationship between pairs of variables.
</div>
<div class="Standard">
features: nodes/ verticies in the graph. We want to try a graph that pairwise connection between features. 
</div>
<div class="Standard">
Learn network structure from data.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-2.1">2.1</a> Markov Network
</h2>
<div class="Standard">
Undirected Graphics Models
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
G & = & \left(V,E\right)\\
V & = & \left\{ 1,\cdots,p\right\} \text{ vertices}\\
E & = & V\times V\text{ edge}
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
Edges are based on conditionally dependent.
</div>
<div class="Standard">
non-edges are conditionally independent.
</div>
<div class="Standard">
For example, <span class="MathJax_Preview"><script type="math/tex">
A
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
B
</script>
</span> conditioned on <span class="MathJax_Preview"><script type="math/tex">
C
</script>
</span>. <span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
P\left(A\mid C\right)\times P\left(B\mid C\right) & = & P\left(A,B\mid C\right)\\
P\left(A,B,C\right) & = & P\left(A\mid C\right)P\left(B\mid C\right)P\left(C\right)
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
Markov Properies
</div>
<ol>
<li>
Pair-wise MP<ol>
<li>
Edge between <span class="MathJax_Preview"><script type="math/tex">
1
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
2
</script>
</span> if <span class="MathJax_Preview"><script type="math/tex">
X_{1}\perp X_{2}\mid X_{3},\cdots,X_{p}
</script>
</span>
</li>

</ol>

</li>
<li>
Local MP<ol>
<li>
No Edge between <span class="MathJax_Preview"><script type="math/tex">
1
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
2
</script>
</span> if <span class="MathJax_Preview"><script type="math/tex">
X_{1}\perp X_{2}\mid N\left(1\right)
</script>
</span>neighbor of <span class="MathJax_Preview"><script type="math/tex">
X_{1}
</script>
</span> (If <span class="MathJax_Preview"><script type="math/tex">
X_{2}\notin N\left(1\right)
</script>
</span> then <span class="MathJax_Preview"><script type="math/tex">
X_{1}\notin N\left(2\right)
</script>
</span>)
</li>
<li>
You just need to consider neighbor.
</li>

</ol>

</li>
<li>
Global MP<ol>
<li>
Define global probability density.
</li>

</ol>

</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-2.2">2.2</a> Gaussian Graphical Model (GGM)
</h2>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

X\sim N\left(0,\Sigma\right)

</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\Theta=\Sigma^{-1}\text{precision matrix}

</script>

</span>

</div>
<ol>
<li>
Zeros in <span class="MathJax_Preview"><script type="math/tex">
\Theta
</script>
</span> <span class="MathJax_Preview"><script type="math/tex">
\rightarrow
</script>
</span>conditional <span class="MathJax_Preview"><script type="math/tex">
\perp
</script>
</span>
</li>
<li>
Zeroes in multi linear regression <span class="MathJax_Preview"><script type="math/tex">
\rightarrow
</script>
</span> conditional <span class="MathJax_Preview"><script type="math/tex">
\perp
</script>
</span>
</li>
<li>
Zeroes in partial correlation <span class="MathJax_Preview"><script type="math/tex">
\rightarrow
</script>
</span> conditional <span class="MathJax_Preview"><script type="math/tex">
\perp
</script>
</span>
</li>

</ol>
<div class="Standard">
Suppose <span class="MathJax_Preview"><script type="math/tex">
X\sim N\left(\mu,\Sigma\right)
</script>
</span>and <span class="MathJax_Preview"><script type="math/tex">
X=\left(Y,Z\right)
</script>
</span> then
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

Y\mid Z\sim N\left(\mu_{x}-\Sigma_{yz}\Sigma_{zz}^{-1}\left(z-\mu_{z}\right),\Sigma_{yy}-\Sigma_{yz}\Sigma_{zz}^{-1}\Sigma_{zy}\right)

</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

X\sim N\left(\mu,\Sigma\right)

</script>

</span>
 <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\begin{bmatrix}Y\\
Z
\end{bmatrix}\sim N\left(\begin{bmatrix}\mu_{y}\\
\mu_{z}
\end{bmatrix},\begin{bmatrix}\Sigma_{xx} & \Sigma_{xz}\\
\Sigma_{zx} & \Sigma_{zz}
\end{bmatrix}\right)

</script>

</span>
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

Y\mid X\sim N\left(\mu_{y},\Sigma_{yy}\right)

</script>

</span>

</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\Sigma_{zz}^{-1}\Sigma_{zy}=-\Theta_{zy}/\Theta_{yy}

</script>

</span>

</div>
<div class="Standard">
Partition matrix inverse formula
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\Theta_{zy}=\begin{bmatrix}0\end{bmatrix}\rightarrow\text{cond. independent}

</script>

</span>

</div>
<ol>
<li>
Precision matrix <span class="MathJax_Preview"><script type="math/tex">
\rightarrow
</script>
</span> any zero: In off diag, <span class="MathJax_Preview"><script type="math/tex">
\Theta_{lk}=0\rightarrow X_{l}\perp X_{k}\mid X_{k},X_{l}
</script>
</span>
</li>
<li>
<span class="MathJax_Preview"><script type="math/tex">
\left\Vert Y-Z\beta\right\Vert _{2}^{2}
</script>
</span> then <span class="MathJax_Preview"><script type="math/tex">
\hat{\beta}=\left(Z^{T}Z\right)^{-1}Z^{T}Y=\hat{\Sigma}_{zz}^{-1}\hat{\Sigma_{zy}}
</script>
</span>
</li>

</ol>
<div class="Standard">
Learning Network Structure
</div>
<div class="Standard">
Learn Edges <span class="MathJax_Preview"><script type="math/tex">
\rightarrow
</script>
</span> conditional <span class="MathJax_Preview"><script type="math/tex">
\perp
</script>
</span> relationship.
</div>
<ol>
<li>
Precision <span class="MathJax_Preview"><script type="math/tex">
\Theta
</script>
</span> <span class="MathJax_Preview"><script type="math/tex">
\rightarrow
</script>
</span> Find zeros in <span class="MathJax_Preview"><script type="math/tex">
\Theta
</script>
</span>. (Sparse inverse covariance estimation) <br/>
Penalize MLE (Graphical Lasso):<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\max_{\Theta}\log\left|\Theta\right|-\dfrac{1}{2}tr\left(X^{T}X\Theta\right)-\lambda\left\Vert \Theta\right\Vert _{1}(\text{sparse penalty})

</script>

</span>

</li>
<li>
Linear Regression<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{\beta}\left\Vert X_{s}-X_{/s}\beta\right\Vert _{2}^{2}+\lambda\left\Vert \beta\right\Vert _{1}

</script>

</span>
If <span class="MathJax_Preview"><script type="math/tex">
\beta_{j}=0
</script>
</span> , no edge between <span class="MathJax_Preview"><script type="math/tex">
x_{s}
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
x_{j}
</script>
</span>. Penalized conditioned MLE.
</li>

</ol>
<h1 class="Section">
<a class="toc" name="toc-Section-3">3</a> Lecture 22 CART (Classification and Regression Tree)
</h1>
<div class="Standard">
Tree, recursive partitions of the data space via binary splits.
</div>
<ol>
<li>
This top down method.
</li>
<li>
Simple models for each split (Constant mean model)
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-3.1">3.1</a> Regression Tree
</h2>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
Y\in\mathbb{R}^{n}
</script>
</span>, <span class="MathJax_Preview"><script type="math/tex">
X\in\mathbb{R}^{n\times p}
</script>
</span> 
</div>
<div class="Standard">
For each split, (a) find a variable to split on (b) find a split point.
</div>
<div class="Standard">
Rule: Samples that follow splitting rule go down left leaf. (Actually, we find nested rectangle in data space.)
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
f\left(X\right) & = & \sum_{m=1}^{M}c_{m}I_{\left(x\in R_{m}\right)}\\
\hat{c}_{m} & = & \bar{Y}_{\left(R_{m}\right)}
\end{eqnarray*}
</script>

</span>
where <span class="MathJax_Preview"><script type="math/tex">
R_{m}
</script>
</span>is one of the terminate node.
</div>
<div class="Standard">
For each node, we defined split point <span class="MathJax_Preview"><script type="math/tex">
s
</script>
</span> and split variable <span class="MathJax_Preview"><script type="math/tex">
j
</script>
</span>.
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
R_{1}\left(j,S\right) & = & \left\{ X\mid x_{j}\le S\right\} \\
R_{2}\left(j,S\right) & = & \left\{ X\mid x_{j}>S\right\} 
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
2 parameters <span class="MathJax_Preview"><script type="math/tex">
s
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
j
</script>
</span>. Our optimization function is<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\min_{S,j}\left[\min_{C_{1}}\sum_{X_{i}\in R_{1}\left(j,s\right)}\left(y_{i}-c_{1}\right)^{2}+\min_{C_{2}}\sum_{X_{i}\in R_{2}\left(j,s\right)}\left(y_{i}-c_{2}\right)^{2}\right]

</script>

</span>

</div>
<div class="Standard">
which is non convex problem.  hard problem but is this hard? NO!
</div>
<div class="Standard">
If we have s and j, the <span class="MathJax_Preview"><script type="math/tex">
\hat{c}_{1,2}
</script>
</span> is just the average <span class="MathJax_Preview"><script type="math/tex">
\left\{ y_{i}\mid x_{i}\in R_{1}\text{ or }x_{i}\in R_{2}\right\} 
</script>
</span>.
</div>
<div class="Standard">
If we have <span class="MathJax_Preview"><script type="math/tex">
c_{1}
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
c_{2}
</script>
</span>:
</div>
<ol>
<li>
If we have <span class="MathJax_Preview"><script type="math/tex">
j
</script>
</span>, how can we solve for <span class="MathJax_Preview"><script type="math/tex">
s
</script>
</span>?
</li>
<li>
Order <span class="MathJax_Preview"><script type="math/tex">
X_{j}
</script>
</span>according to <span class="MathJax_Preview"><script type="math/tex">
y
</script>
</span> (pre-step complexity <span class="MathJax_Preview"><script type="math/tex">
O\left(n\log n\right)
</script>
</span>)
</li>
<li>
scan overall possible <span class="MathJax_Preview"><script type="math/tex">
s
</script>
</span>. 
</li>

</ol>
<div class="Standard">
Then the overall complexity is <span class="MathJax_Preview"><script type="math/tex">
O\left(np\right)+O\left(n\log n\right)
</script>
</span> which is faster than least square.
</div>
<div class="Standard">
Grew the full tree <span class="MathJax_Preview"><script type="math/tex">
M=n
</script>
</span> quite similar as <span class="MathJax_Preview"><script type="math/tex">
1NN
</script>
</span>.
</div>
<div class="Standard">
Build a tree with <span class="MathJax_Preview"><script type="math/tex">
M
</script>
</span> terminates nodes . 
</div>
<div class="Standard">
If we ave <span class="MathJax_Preview"><script type="math/tex">
X^{new}
</script>
</span> , how do we predict <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span>? We follow the same splitting along to the terminate nodes to predict the value.
</div>
<div class="Standard">
Cost complexity Pruning
</div>
<div class="Standard">
Idea: grow full tree and then &ldquo;prune&rdquo; tree by removing week links.
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">
\begin{eqnarray*}
L\left(T\right) & = & \sum_{i=1}^{M}\dfrac{1}{n_{m}}\sum_{x_{i}\in R_{m}}\left(y_{i}-\hat{c}_{m}\right)^{2}\\
CP\left(\alpha\right) & = & \sum_{m=1}^{M}n_{m}L_{m}\left(T\right)+\alpha M
\end{eqnarray*}
</script>

</span>

</div>
<div class="Standard">
when <span class="MathJax_Preview"><script type="math/tex">
\alpha=0
</script>
</span>, full tree while <span class="MathJax_Preview"><script type="math/tex">
\alpha\rightarrow\infty
</script>
</span>, then small tree. 
</div>
<div class="Standard">
How to choose <span class="MathJax_Preview"><script type="math/tex">
\alpha
</script>
</span>? K-Fold CV.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-3.2">3.2</a> Classification Tree
</h2>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
Y\in C_{1},\cdots,C_{k}
</script>
</span> 
</div>
<div class="Standard">
Loss function is defined 
</div>
<ol>
<li>
Error within each region<ol>
<li>
Missclassification (Discrete loss function) <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{p}_{mk}=\dfrac{1}{n_{m}}\sum_{x_{i}\in R_{m}}I\left(y_{i}=k\right)

</script>

</span>
Predict <span class="MathJax_Preview">
<script type="math/tex;mode=display">

k=arg\max_{k}\hat{p}_{mk}

</script>

</span>

</li>
<li>
Gini Error<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\sum_{k\ne k}\hat{p}_{mk}\left(1-\hat{p}_{mk}\right)

</script>

</span>
which measure multinational variance.
</li>
<li>
Cross Entropy Error<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\sum_{k=1}^{K}\hat{p}_{mk}\log\left(\hat{p}_{mk}\right)

</script>

</span>

</li>
<li>
Gini and Entropy loss function enforce purity of the class.
</li>

</ol>

</li>

</ol>
<div class="Standard">
What if some <span class="MathJax_Preview"><script type="math/tex">
X
</script>
</span> are category variables?
</div>
<div class="Standard">
Tree will split the category according to <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span> values. Tree here is easy than other continuous classifier.
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-4">4</a> Lecture 23
</h1>
<div class="Standard">
CART: We are trying to find &ldquo;rectangle&rdquo; in feature spaces and fit supervised model in each <span class="MathJax_Preview"><script type="math/tex">
\mathbb{R}_{m}
</script>
</span>
</div>
<div class="Standard">
Recursively fit binary splits. (nested structure gives us dendograms)
</div>
<div class="Standard">
For regression, we typically fit a constant mean model. loss function: squared errors.
</div>
<div class="Standard">
For Classification, we want high proportion of one class (or node purity). loss function: missclassification, gini, and cross entropy.
</div>
<div class="Standard">
Building trees
</div>
<div class="Standard">
Splitting Rules: (Variable <span class="MathJax_Preview"><script type="math/tex">
j
</script>
</span> and splitting point <span class="MathJax_Preview"><script type="math/tex">
s
</script>
</span>) 
</div>
<div class="Standard">
Tree in Linear model doesn’t work well.
</div>
<div class="Standard">
Tree are non linear model.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-4.1">4.1</a> Ensemble Learning
</h2>
<div class="Standard">
Idea: combine strengths of several &ldquo;weak learner&rdquo; 
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{y}=\sum_{m=1}^{M}\hat{w}_{n}\hat{f}_{m}\left(x\right)

</script>

</span>

</div>
<div class="Standard">
where <span class="MathJax_Preview"><script type="math/tex">
\hat{f}_{m}\left(x\right)
</script>
</span> is weak learner. 
</div>
<h3 class="Subsubsection">
<a class="toc" name="toc-Subsubsection-4.1.1">4.1.1</a> Bagging
</h3>
<div class="Standard">
(tree) high variance
</div>
<div class="Standard">
Law of large number: <span class="MathJax_Preview"><script type="math/tex">
\bar{X}_{n}\rightarrow\mu
</script>
</span> as <span class="MathJax_Preview"><script type="math/tex">
n\rightarrow\infty
</script>
</span>.
</div>
<div class="Standard">
What if we have <span class="MathJax_Preview"><script type="math/tex">
B
</script>
</span> training sets and <span class="MathJax_Preview"><script type="math/tex">
T\left(x^{b}\right)
</script>
</span> then <span class="MathJax_Preview"><script type="math/tex">
\hat{f}\left(x\right)=\dfrac{1}{B}\sum_{b=1}^{B}T\left(x^{b}\right)
</script>
</span>.
</div>
<div class="Standard">
However, Don’t have <span class="MathJax_Preview"><script type="math/tex">
B
</script>
</span> training sets? Bootstrap! 
</div>
<div class="Standard">
Bootstrap re sampling method 
</div>
<div class="Standard">
We take <span class="MathJax_Preview"><script type="math/tex">
n
</script>
</span> samples with replacement from data of size <span class="MathJax_Preview"><script type="math/tex">
n
</script>
</span>.
</div>
<div class="Standard">
Suppose <span class="MathJax_Preview"><script type="math/tex">
X_{n\times p}^{b}
</script>
</span> is the bootstrap sample set. (some rows in twice or more. some don’t appear)
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

P\left(i\in B\right)=1-\left(1-\dfrac{1}{n}\right)^{n}\approx1-e^{-1}=0.632

</script>

</span>

</div>
<h1 class="Section">
<a class="toc" name="toc-Section-5">5</a> Lecture 24
</h1>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.1">5.1</a> Bagging
</h2>
<div class="Standard">
Boost strap aggregation.
</div>
<div class="Standard">
Sampling <span class="MathJax_Preview"><script type="math/tex">
n
</script>
</span> total observations with replacement.
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
X_{n\times p}
</script>
</span>or <span class="MathJax_Preview"><script type="math/tex">
X^{b}
</script>
</span> is the boost strap copy of the original dataset such that the rows are from the boost strap sample. (Mimic the random sampling process)
</div>
<div class="Standard">
Fit our learner many <span class="MathJax_Preview"><script type="math/tex">
B
</script>
</span> times to <b><span class="MathJax_Preview"><script type="math/tex">
B
</script>
</span></b> boost strap samples <span class="MathJax_Preview"><script type="math/tex">
X^{b}
</script>
</span>. In bagging, we take average<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{f}_{\text{bag}}=\dfrac{1}{B}\sum_{b=1}^{B}\hat{f}^{b}\left(X^{b}\right)

</script>

</span>

</div>
<div class="Standard">
For regression tree, we would like to<span class="MathJax_Preview">
<script type="math/tex;mode=display">

MSE\left(\hat{f}_{tree}\right)>MSE\left(\hat{f}_{bag}\right)

</script>

</span>

</div>
<div class="Standard">
Key Point:
</div>
<ol>
<li>
Averaging reduce variance! (Law of Large Number)
</li>
<li>
Does bagging over fit? No, but if individual learners overfit, then bagging might overfit.
</li>
<li>
No interpretation.
</li>

</ol>
<div class="Standard">
Effect of Bagging
</div>
<ol>
<li>
Tree Model: Bagging will not decrease bias but decrease variance of Tree Regression.
</li>
<li>
Linear Model: Both Bias and variance are unchanged, since linear model’s BLUE model or minimal variance models.
</li>
<li>
However, for sparse regression with penalty bagging will take effect.
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.2">5.2</a> OOB (Out of Bag) Error
</h2>
<div class="Standard">
Each booststrap sample approximates retain 64% percent of the original dataset.
</div>
<div class="Standard">
This is very convenient for model selection or tuning parameters. We can tune parameters on the obs left out of booststrap sample.
</div>
<div class="Standard">
Fit to <span class="MathJax_Preview"><script type="math/tex">
X^{b}
</script>
</span> and get prediction error from the left out <span class="MathJax_Preview"><script type="math/tex">
X^{b^{C}}
</script>
</span>.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.3">5.3</a> Boosting (1996)
</h2>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{f}=\sum_{m=1}^{M}\alpha_{m}\hat{f}_{m}\left(x\right)

</script>

</span>

</div>
<div class="Standard">
Sequential additive models &ldquo;week&rdquo; learners 
</div>
<div class="Standard">
Greedy manner.
</div>
<div class="Standard">
Key strategies: Fit to the residuals of data matrix. Adjust the observation weights.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.4">5.4</a> Adaboost (Discrete Verizon)
</h2>
<div class="Standard">
Suppose we have <span class="MathJax_Preview"><script type="math/tex">
Y\in\left\{ -1,1\right\} 
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
X_{n\times p}
</script>
</span>. 
</div>
<ol>
<li>
Initialize weights as <span class="MathJax_Preview"><script type="math/tex">
w_{i}=\dfrac{1}{n}
</script>
</span>
</li>
<li>
Repeat for <span class="MathJax_Preview"><script type="math/tex">
m=1\cdots M
</script>
</span><ol>
<li>
Fit a classifier <span class="MathJax_Preview"><script type="math/tex">
f_{m}\left(x\right)
</script>
</span> with weights to <span class="MathJax_Preview"><script type="math/tex">
Y
</script>
</span>. (Usually 2 stup tree)
</li>
<li>
Calculate weighted missclassification error<span class="MathJax_Preview">
<script type="math/tex;mode=display">

err_{m}=\dfrac{\sum_{i=1}^{m}w_{i}I\left(y_{i}\ne f_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{m}w_{i}}

</script>

</span>
and calculate the <span class="MathJax_Preview"><script type="math/tex">
\alpha_{m}
</script>
</span> (the log odds of weighted errors) defined as 
</li>
<li>
Now we update our weights as<span class="MathJax_Preview">
<script type="math/tex;mode=display">

w_{i}:=w_{i}\exp\left[\alpha_{m}I\left(y_{i}\ne f_{m}\left(x_{i}\right)\right)\right]

</script>

</span>
and normalize our weights such that <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\sum_{i=1}^{n}w_{i}=1

</script>

</span>

</li>

</ol>

</li>
<li>
Output our final classifier as  <span class="MathJax_Preview">
<script type="math/tex;mode=display">

sign\left[\sum_{m=1}^{M}\alpha_{m}f_{m}\left(x\right)\right]

</script>

</span>

</li>

</ol>
<div class="Standard">
The above process means the following
</div>
<ol>
<li>
good classifier <span class="MathJax_Preview"><script type="math/tex">
f_{m}
</script>
</span> has larger coefficients <span class="MathJax_Preview"><script type="math/tex">
\alpha_{m}
</script>
</span>
</li>
<li>
obs misclassified by <span class="MathJax_Preview"><script type="math/tex">
\hat{f}_{m}\left(x_{i}\right)
</script>
</span>, then we update the observation weights as <span class="MathJax_Preview"><script type="math/tex">
w_{i}:=w_{i}\exp\left(\alpha_{m}\right)
</script>
</span> such that the weights will increase on misclassified obs.
</li>
<li>
Weights are adjusted to up weight all misclassified observations.
</li>
<li>
Adjustment is such that <span class="MathJax_Preview"><script type="math/tex">
\hat{f}_{m}
</script>
</span> would yield 50% class accuracy (fitting to the residuals).
</li>

</ol>
<div class="Standard">
Some conclusions about Adaboost
</div>
<ol>
<li>
Adaboost minimize the loss function <span class="MathJax_Preview">
<script type="math/tex;mode=display">

L\left(y,f\left(x\right)\right)=\exp\left(-yf\left(x\right)\right)

</script>

</span>
When <span class="MathJax_Preview"><script type="math/tex">
f\left(x_{i}\right)y_{i}>1
</script>
</span>, correctly classified. When <span class="MathJax_Preview"><script type="math/tex">
f\left(x_{i}\right)y_{i}\in\left(0,1\right)
</script>
</span>, correctly classified but closely. When <span class="MathJax_Preview"><script type="math/tex">
f\left(x_{i}\right)y_{i}<0
</script>
</span>, missclassified. 
</li>
<li>
Adaboost is doing a sequential greedy descent for the exponential loss.
</li>
<li>
exponential loss also minimizes <span class="MathJax_Preview">
<script type="math/tex;mode=display">

\dfrac{1}{2}\log\left(\dfrac{P\left(Y=1\right)}{P\left(Y=-1\right)}\right)

</script>

</span>

</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-5.5">5.5</a> Forward Stagewise Additive Modeling
</h2>
<div class="Standard">
For <span class="MathJax_Preview"><script type="math/tex">
m=1\cdots M
</script>
</span>
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\left(\alpha_{m},\gamma_{m}\right)=\overset{argmin}{\alpha,\gamma}\sum_{i=1}^{n}L\left(y_{i},f_{m-1}\left(x_{i}\right)+\alpha g\left(x_{i},\gamma\right)\right)

</script>

</span>

</div>
<div class="Standard">
and update the model as <span class="MathJax_Preview">
<script type="math/tex;mode=display">

f_{m}\left(x\right)=f_{m-1}\left(x\right)+\alpha_{m}g\left(x,\gamma_{m}\right)

</script>

</span>

</div>
<div class="Standard">
Exponentail loss for adaboost
</div>
<div class="Standard">
Logistics loss for Logitboost
</div>
<div class="Standard">
Why 2 stump tree?
</div>
<ol>
<li>
2 stump: split on only 1 feature
</li>
<li>
more variation of the 2 stump tree.
</li>

</ol>
<h1 class="Section">
<a class="toc" name="toc-Section-6">6</a> Lecture 25
</h1>
<div class="Standard">
Can bagging overfit? No! (theory)
</div>
<div class="Standard">
Overfitting: Variance of estimations.
</div>
<div class="Standard">
Bias remains the same.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.1">6.1</a> Boosting
</h2>
<div class="Standard">
Additive ensemble of week learners. Fit sequentially.
</div>
<div class="Standard">
Slow learning.
</div>
<div class="Standard">
Forward Additive Stagewise Alg:
</div>
<div class="Standard">
For <span class="MathJax_Preview"><script type="math/tex">
m=1,\cdots,M
</script>
</span>
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\left(\alpha_{m},\gamma_{m}\right)=\overset{argmin}{\alpha,\gamma}\sum_{i=1}^{n}L\left(y_{i},\underbrace{f_{m-1}\left(x_{i}\right)}_{\text{fitting to the residuals}}+\alpha g\left(x,\gamma\right)\right)

</script>

</span>

</div>
<div class="Standard">
(rarely use)<span class="MathJax_Preview">
<script type="math/tex;mode=display">

f_{m}\left(x\right)=f_{m-1}\left(x\right)+\alpha_{m}g\left(x,\gamma_{m}\right)

</script>

</span>

</div>
<div class="Standard">
Output<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{f}_{m}\left(x\right)=\sum_{m=1}^{M}\hat{\alpha}_{m}\hat{g}\left(x,\gamma_{m}\right)

</script>

</span>

</div>
<div class="Standard">
Generic Loss function <span class="MathJax_Preview"><script type="math/tex">
L\left(\right)
</script>
</span>
</div>
<div class="Standard">
Generic &ldquo;Learner&rdquo; <span class="MathJax_Preview"><script type="math/tex">
g\left(\right)
</script>
</span> 
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
\eta
</script>
</span> tuning parameter
</div>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

f_{m}\left(x\right)=f_{m-1}\left(x\right)+\eta\alpha_{m}g\left(x,\gamma_{m}\right)

</script>

</span>

</div>
<div class="Standard">
where <span class="MathJax_Preview"><script type="math/tex">
\eta
</script>
</span> is the learning rate <span class="MathJax_Preview"><script type="math/tex">
\in\left(0,1\right)
</script>
</span>. And <span class="MathJax_Preview"><script type="math/tex">
\eta\downarrow
</script>
</span> slow learner (Prefer) and <span class="MathJax_Preview"><script type="math/tex">
\eta\uparrow
</script>
</span> faster learner (The faster the learner the higher tendency of overfit).
</div>
<div class="Standard">
<span class="MathJax_Preview"><script type="math/tex">
L\left(y,f\left(x\right)\right)
</script>
</span> (Data Adaptive)
</div>
<div class="Standard">
Boosting: less likely to overfit and generate better. 
</div>
<div class="Standard">
The Key idea is the slowly fitting to residuals.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.2">6.2</a> Boosting Algs (Adaboost)
</h2>
<div class="Standard">
Adaboost
</div>
<ol>
<li>
Exponential Loss 
</li>
<li>
Tree stumps as learner
</li>

</ol>
<div class="Standard">
Logit Boost
</div>
<ol>
<li>
Logistic loss: binomial deviation loss
</li>

</ol>
<div class="Standard">
Gradient Tree Boosting
</div>
<ol>
<li>
Any loss function.
</li>
<li>
Learner trees.
</li>
<li>
Useful for regression with non linear data.
</li>

</ol>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.3">6.3</a> Types of Boosters
</h2>
<ol>
<li>
Discrete <span class="MathJax_Preview"><script type="math/tex">
y\in\left\{ -1,1\right\} 
</script>
</span><ol>
<li>
(classification) Adaboost on board.
</li>
<li>
Majority Votes.
</li>

</ol>

</li>
<li>
Real <span class="MathJax_Preview"><script type="math/tex">
y
</script>
</span> as probabilities (classification)<ol>
<li>
Logit Boost
</li>

</ol>

</li>
<li>
Gentle<ol>
<li>
Gradient Boosting
</li>
<li>
Replace <span class="MathJax_Preview"><script type="math/tex">
\left(1\right)
</script>
</span> with stepping in the direction of <span class="MathJax_Preview"><script type="math/tex">
-\nabla l\left(\right)
</script>
</span>.
</li>
<li>
Works very well for regression.
</li>

</ol>

</li>

</ol>
<div class="Standard">
Can Boosting overfit?
</div>
<div class="Standard">
Yes! but hard to with slow learners. 
</div>
<div class="Standard">
Parameter tunings is <span class="MathJax_Preview"><script type="math/tex">
\eta
</script>
</span> and <span class="MathJax_Preview"><script type="math/tex">
M
</script>
</span>. 
</div>
<ol>
<li>
Validation set (CV)
</li>
<li>
Bagging Calculate the OOB error to decide when to stop <span class="MathJax_Preview"><script type="math/tex">
\left(M\right)
</script>
</span>.
</li>

</ol>
<div class="Standard">
How can we interpret the boosters?
</div>
<div class="Standard">
Difficult for interpretability. 
</div>
<div class="Standard">
For Trees: <span class="MathJax_Preview"><script type="math/tex">
I\left(T\right)=\dfrac{1}{M}\sum_{m=1}^{M}RSSI_{m}^{2}I\left(\text{split on \ensuremath{X}}\right)
</script>
</span>, Improvement in RSS after split.
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-6.4">6.4</a> Random Forest 
</h2>
<div class="Standard">
Idea: Improve prediction error, <span class="MathJax_Preview"><script type="math/tex">
MSE=\underbrace{Var}_{\text{reduce as possible}}+\underbrace{Bias^{2}}_{\text{can reducce}}
</script>
</span>.
</div>
<div class="Standard">
Bagging has the same reduce variance property. 
</div>
<div class="Standard">
Are our trees independent? No!
</div>
<div class="Standard">
Will the variance decrease to <span class="MathJax_Preview"><script type="math/tex">
0
</script>
</span>? We can correlation between the different trees.
</div>
<div class="Standard">
Boosting:
</div>
<div class="Standard">
Are tree independent? No, since we fit sequentially.
</div>
<div class="Standard">
Idea: RF we want iid trees.
</div>
<div class="Standard">
Key Idea: The best ensembles are from approximately independent learners!
</div>
<div class="Standard">
Idea: We are going to perturb both <span class="MathJax_Preview"><script type="math/tex">
n
</script>
</span>and <span class="MathJax_Preview"><script type="math/tex">
p
</script>
</span> space!
</div>
<div class="Standard">
RF Algorithm: (B, <span class="MathJax_Preview"><script type="math/tex">
m
</script>
</span> # number of splits each tree, <span class="MathJax_Preview"><script type="math/tex">
\left|T\right|
</script>
</span> tree size)
</div>
<div class="Standard">
For <span class="MathJax_Preview"><script type="math/tex">
b=1\cdots B
</script>
</span>
</div>
<ol>
<li>
Boost strap sample of size <span class="MathJax_Preview"><script type="math/tex">
n
</script>
</span>. (Perturb obs space boosting)
</li>
<li>
Grow a <span class="MathJax_Preview"><script type="math/tex">
RF
</script>
</span> tree <span class="MathJax_Preview"><script type="math/tex">
T_{b}
</script>
</span>.
</li>
<li>
Select <span class="MathJax_Preview"><script type="math/tex">
m
</script>
</span> vars randomly out of <span class="MathJax_Preview"><script type="math/tex">
p
</script>
</span> variables. (uniformly select <span class="MathJax_Preview"><script type="math/tex">
5
</script>
</span> variables)
</li>
<li>
Find the best split and split point out of the <span class="MathJax_Preview"><script type="math/tex">
m
</script>
</span> vars.
</li>
<li>
Split into 2 daughters.
</li>
<li>
Recursively repeat until converge.
</li>

</ol>
<div class="Standard">
RF is faster to compute, because of perturbing in the feature spaces.
</div>
<h1 class="Section">
<a class="toc" name="toc-Section-7">7</a> Lecture 26
</h1>
<div class="Standard">
Random Forest
</div>
<div class="Standard">
Idea: Ensemble &ldquo;de-correlated&rdquo; trees to get as close as possible to iid.
</div>
<div class="Standard">
Algorithms: 
</div>
<ol>
<li>
Bootstrap for <span class="MathJax_Preview"><script type="math/tex">
b=1\cdots B
</script>
</span>
</li>
<li>
Build a RF tree<ol>
<li>
For each split, we choose a subset <span class="MathJax_Preview"><script type="math/tex">
m
</script>
</span> out of <span class="MathJax_Preview"><script type="math/tex">
p
</script>
</span> variables (Random).
</li>
<li>
Find the best split and split points (Use Brute force)
</li>
<li>
Split into <span class="MathJax_Preview"><script type="math/tex">
2
</script>
</span> daughters.
</li>

</ol>

</li>

</ol>
<div class="Standard">
Computational complexity: relatively fast 
</div>
<ol>
<li>
faster than bagging, bagging is building a tree with all variables
</li>
<li>
Boosting time larger than Random Forest, because rf can utilize parallel computing.
</li>

</ol>
<div class="Standard">
How well does this work?<span class="MathJax_Preview">
<script type="math/tex;mode=display">

MSE=Bias^{2}+Var

</script>

</span>

</div>
<div class="Standard">
Bias dependents on the size of <span class="MathJax_Preview"><script type="math/tex">
\left|T\right|
</script>
</span>. so how to choose the size of <span class="MathJax_Preview"><script type="math/tex">
\left|T\right|
</script>
</span>.
</div>
<div class="Standard">
When <span class="MathJax_Preview"><script type="math/tex">
\left|T\right|\approx n
</script>
</span>, then the random forest approximate 1NN methods.
</div>
<div class="Standard">
Variance is low: Bagging &gt; RF (Bagging are correlated while RF is reduced by iid sampling a subset from features).
</div>
<div class="Standard">
When <span class="MathJax_Preview"><script type="math/tex">
p\ge n
</script>
</span>, RF is bad because we will miss relevant features by noises.
</div>
<div class="Standard">
Random Forest overfitting? No
</div>
<h2 class="Subsection">
<a class="toc" name="toc-Subsection-7.1">7.1</a> Model Stacking
</h2>
<div class="Standard">
<span class="MathJax_Preview">
<script type="math/tex;mode=display">

\hat{f}=\sum_{m=1}^{M}\omega_{m}\underbrace{\hat{f}_{m}}_{\text{prediction from any base learners}}

</script>

</span>

</div>
<div class="Standard">
parameters <span class="MathJax_Preview"><script type="math/tex">
w_{n}
</script>
</span> Linear models (can also be non-linear).
</div>
<div class="Standard">
Key ensemble takeaways
</div>
<ol>
<li>
Average, Average reduce variance. (reduce variance) (If done intelligent, averaging always improve.)
</li>
<li>
Randomize, Randomizing reduces correlations among ensemble members and also reduces variation.
</li>

</ol>

<hr class="footer"/>
<div class="footer" id="generated-by">
Document generated by <a href="http://elyxer.nongnu.org/">eLyXer 1.2.5 (2013-03-10)</a> on <span class="create-date">2014-12-06T18:09:09.421000</span>
</div>
</div>
</body>
</html>
